{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052aaffa-d670-4d3f-bed9-c07e12c063cf",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "732c2405-3fcb-489a-b931-15d9a7eb7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from qucumber.nn_states import DensityMatrix\n",
    "from qucumber.nn_states import ComplexWaveFunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "import qucumber.utils.unitaries as unitaries\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.cplx as cplx\n",
    "import qucumber.utils.data as data\n",
    "from qucumber.observables import ObservableBase, to_pm1\n",
    "from qucumber.observables.pauli import flip_spin\n",
    "import qucumber\n",
    "\n",
    "from qulacs.gate import Pauli\n",
    "\n",
    "with open('./params_setting.yaml', 'r') as yml:\n",
    "    params = yaml.safe_load(yml)\n",
    "    \n",
    "# quantum circuit parameter\n",
    "n_qubit = params[\"circuit_info\"][\"n_qubit\"]\n",
    "n_data = params[\"circuit_info\"][\"n_data\"]\n",
    "each_n_shot = int(n_data / 3**n_qubit)\n",
    "state_name = params[\"circuit_info\"][\"state_name\"]\n",
    "error_model = params[\"circuit_info\"][\"error_model\"]\n",
    "error_rate = params[\"circuit_info\"][\"error_rate\"]\n",
    "# RBM architecture parameter\n",
    "n_visible_unit = params[\"architecture_info\"][\"n_visible_unit\"]\n",
    "n_hidden_unit = params[\"architecture_info\"][\"n_hidden_unit\"] \n",
    "n_aux_unit = params[\"architecture_info\"][\"n_aux_unit\"]\n",
    "# train parameter\n",
    "lr = params[\"train_info\"][\"lr\"]\n",
    "pbs = params[\"train_info\"][\"positive_batch_size\"]\n",
    "nbs = params[\"train_info\"][\"negative_batch_size\"]\n",
    "n_gibbs_step = params[\"train_info\"][\"n_gibbs_step\"]\n",
    "period = 1\n",
    "epochs = params[\"train_info\"][\"n_epoch\"]\n",
    "lr_drop_epoch = params[\"train_info\"][\"lr_drop_epoch\"]\n",
    "lr_drop_factor = params[\"train_info\"][\"lr_drop_factor\"]\n",
    "seed = params[\"train_info\"][\"seed\"]\n",
    "# sampling parameter\n",
    "n_sampling = params[\"sampling_info\"][\"n_sample\"]\n",
    "n_copy = params[\"sampling_info\"][\"n_copy\"]\n",
    "# data path info\n",
    "train_data_path = f\"./data/{noise_model}/error_prob_{100*error_rate}%/num_of_data_{n_data}\"\n",
    "ideal_state_path = f\"./target_state\"\n",
    "\n",
    "# settings\n",
    "## warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## seaborn layout\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "## seed\n",
    "def seed_settings(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    qucumber.set_random_seed(seed, cpu=True, gpu=False)\n",
    "\n",
    "seed_settings(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117e73d-0af0-430e-8a7c-3b2d47ef109a",
   "metadata": {},
   "source": [
    "## caluculate ideal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d800bd19-5c39-4fcf-9f48-49451f4574a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate ideal state\n",
    "is_ideal_state_file = os.path.isfile(ideal_state_path + \"/*.txt\")\n",
    "if is_ideal_state_file:\n",
    "    print(\"ideal state data is exsisted !\")\n",
    "else:\n",
    "    print(\"caluculate ideal state data ...\")\n",
    "    subprocess.run(\"python caluculate_ideal_state.py\", shell=True)\n",
    "    print(\"ideal state data is ready !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a9677-cd6b-41d8-92a1-47a74a019756",
   "metadata": {},
   "source": [
    "## generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "818975eb-16fe-4d57-b770-abadc051b88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train_data_file = os.path.isfile(train_data_path + \"/*.txt\")\n",
    "is_train_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8227038-435d-419b-a558-f30b8643125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate directries & train data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/GitHub/NQS4QEM/Bell/generate_dataset.py\", line 599, in <module>\n",
      "    main()\n",
      "  File \"/content/GitHub/NQS4QEM/Bell/generate_dataset.py\", line 584, in main\n",
      "    ideal_state_vector_df[\"Re\"] = ideal_state_vector_df[\"Re\"].apply(lambda x: \" \".join(x))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pandas/core/series.py\", line 4433, in apply\n",
      "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\", line 1082, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\", line 1137, in apply_standard\n",
      "    mapped = lib.map_infer(\n",
      "  File \"pandas/_libs/lib.pyx\", line 2870, in pandas._libs.lib.map_infer\n",
      "  File \"/content/GitHub/NQS4QEM/Bell/generate_dataset.py\", line 584, in <lambda>\n",
      "    ideal_state_vector_df[\"Re\"] = ideal_state_vector_df[\"Re\"].apply(lambda x: \" \".join(x))\n",
      "TypeError: can only join an iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is ready !\n"
     ]
    }
   ],
   "source": [
    "# generate train data\n",
    "is_train_data_file = os.path.isfile(train_data_path + \"/*.txt\")\n",
    "if is_train_data_file:\n",
    "    print(\"train data is exsisted !\")\n",
    "else:\n",
    "    print(\"generate directries & train data ...\")\n",
    "    os.makedirs(train_data_path, exist_ok = True)\n",
    "    subprocess.run(\"python generate_dataset.py\", shell=True)\n",
    "    print(\"train data is ready !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca5da5-599e-4904-b098-e7674898d406",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cabc549-042c-4d1d-874e-8442ae6c1015",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m meas_pattern \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_DM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/measurement_pattern.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m meas_label \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_data_DM(train_data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/measurement_label.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m meas_result \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_data_DM(train_data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/measurement_result.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/qucumber/utils/data.py:85\u001b[0m, in \u001b[0;36mload_data_DM\u001b[0;34m(tr_samples_path, tr_mtx_real_path, tr_mtx_imag_path, tr_bases_path, bases_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Load the data required for training.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m:param tr_samples_path: The path to the training data.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m:rtype: list\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 85\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_samples_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble)\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tr_mtx_real_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     mtx_real \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     90\u001b[0m         np\u001b[38;5;241m.\u001b[39mloadtxt(tr_mtx_real_path, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble\n\u001b[1;32m     91\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/npyio.py:1148\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# read data in chunks and fill it into an array via resize\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# over-allocating and shrinking the array later may be faster but is\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# probably not relevant compared to the cost of actually reading and\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# converting the data\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m read_data(_loadtxt_chunksize):\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x, dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36mloadtxt.<locals>.read_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong number of columns at line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[38;5;241m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[38;5;241m=\u001b[39m [conv(val) \u001b[38;5;28;01mfor\u001b[39;00m (conv, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[38;5;241m=\u001b[39m pack_items(items, packing)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong number of columns at line \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[38;5;241m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (conv, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[38;5;241m=\u001b[39m pack_items(items, packing)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/npyio.py:736\u001b[0m, in \u001b[0;36m_getconv.<locals>.floatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0x\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m.\u001b[39mfromhex(x)\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'X'"
     ]
    }
   ],
   "source": [
    "meas_pattern = data.load_data_DM(train_data_path + \"/measurement_pattern.txt\")\n",
    "meas_label = data.load_data_DM(train_data_path + \"/measurement_label.txt\")\n",
    "meas_result = data.load_data_DM(train_data_path + \"/measurement_result.txt\")\n",
    "ideal_rho_re = data.load_data_DM(ideal_state_path + \"/rho_real.txt\")\n",
    "ideal_rho_im = data.load_data_DM(ideal_state_path + \"/rho_imag.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00583a-d830-4ad8-97f8-6d0791e3dd70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052aaffa-d670-4d3f-bed9-c07e12c063cf",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "732c2405-3fcb-489a-b931-15d9a7eb7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from qucumber.nn_states import DensityMatrix\n",
    "from qucumber.nn_states import ComplexWaveFunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "import qucumber.utils.unitaries as unitaries\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.cplx as cplx\n",
    "import qucumber.utils.data as data\n",
    "from qucumber.observables import ObservableBase, to_pm1\n",
    "from qucumber.observables.pauli import flip_spin\n",
    "import qucumber\n",
    "\n",
    "from qulacs.gate import Pauli\n",
    "\n",
    "with open('./params_setting.yaml', 'r') as yml:\n",
    "    params = yaml.safe_load(yml)\n",
    "    \n",
    "# quantum circuit parameter\n",
    "n_qubit = params[\"circuit_info\"][\"n_qubit\"]\n",
    "each_n_shot = params[\"circuit_info\"][\"each_n_shot\"]\n",
    "state_name = params[\"circuit_info\"][\"state_name\"]\n",
    "error_model = params[\"circuit_info\"][\"error_model\"]\n",
    "error_rate = params[\"circuit_info\"][\"error_rate\"]\n",
    "# RBM architecture parameter\n",
    "n_visible_unit = params[\"architecture_info\"][\"n_visible_unit\"]\n",
    "n_hidden_unit = params[\"architecture_info\"][\"n_hidden_unit\"] \n",
    "n_aux_unit = params[\"architecture_info\"][\"n_aux_unit\"]\n",
    "# train parameter\n",
    "lr = params[\"train_info\"][\"lr\"]\n",
    "pbs = params[\"train_info\"][\"positive_batch_size\"]\n",
    "nbs = params[\"train_info\"][\"negative_batch_size\"]\n",
    "n_gibbs_step = params[\"train_info\"][\"n_gibbs_step\"]\n",
    "period = 1\n",
    "epoch = params[\"train_info\"][\"n_epoch\"]\n",
    "lr_drop_epoch = params[\"train_info\"][\"lr_drop_epoch\"]\n",
    "lr_drop_factor = params[\"train_info\"][\"lr_drop_factor\"]\n",
    "seed = params[\"train_info\"][\"seed\"]\n",
    "# sampling parameter\n",
    "n_sampling = params[\"sampling_info\"][\"n_sample\"]\n",
    "n_copy = params[\"sampling_info\"][\"n_copy\"]\n",
    "# data path info\n",
    "environment = \"local\"\n",
    "if environment == \"local\":\n",
    "    train_data_path = f\"./data/{error_model}/error_prob_{100*error_rate}%/num_of_data_{each_n_shot}/\"\n",
    "    ideal_state_path = f\"./target_state/\"\n",
    "if environment == \"colab\":\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "    rive_path = \"/content/drive/MyDrive/NQS4QEM/Bell\"\n",
    "    train_data_path = drive_path + f\"/data/{error_model}/error_prob_{100*error_rate}%/num_of_data_{each_n_shot}/\"\n",
    "    ideal_state_path = drive_path + f\"/target_state/\"\n",
    "\n",
    "# settings\n",
    "## warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## seaborn layout\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "## seed\n",
    "def seed_settings(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    qucumber.set_random_seed(seed, cpu=True, gpu=False)\n",
    "\n",
    "seed_settings(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117e73d-0af0-430e-8a7c-3b2d47ef109a",
   "metadata": {},
   "source": [
    "## caluculate ideal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d800bd19-5c39-4fcf-9f48-49451f4574a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ideal state data is exsisted !\n"
     ]
    }
   ],
   "source": [
    "# calculate ideal state\n",
    "is_ideal_state_file = os.path.exists(ideal_state_path)\n",
    "if is_ideal_state_file:\n",
    "    print(\"ideal state data is exsisted !\")\n",
    "else:\n",
    "    print(\"caluculate ideal state data ...\")\n",
    "    subprocess.run(\"python caluculate_ideal_state.py\", shell=True)\n",
    "    print(\"ideal state data is ready !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a9677-cd6b-41d8-92a1-47a74a019756",
   "metadata": {},
   "source": [
    "## generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8227038-435d-419b-a558-f30b8643125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is exsisted !\n"
     ]
    }
   ],
   "source": [
    "# generate train data\n",
    "is_train_data_file = os.path.exists(train_data_path)\n",
    "if is_train_data_file:\n",
    "    print(\"train data is exsisted !\")\n",
    "else:\n",
    "    print(\"generate directries & train data ...\")\n",
    "    os.makedirs(train_data_path, exist_ok = True)\n",
    "    subprocess.run(\"python generate_dataset.py\", shell=True)\n",
    "    print(\"train data is ready !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90a855-f64f-4d31-9dff-25973fd5bbfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## estimate observable expectation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f385a22-df7a-49b0-880d-39dadb7f6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralPauliDistill(ObservableBase):\n",
    "    def __init__(self, pauli_dict: dict, m: int) -> None:\n",
    "        self.name = \"distilled_pauli\"\n",
    "        self.symbol = \"distilled_general_pauli\"\n",
    "        self.pauli_dict = pauli_dict\n",
    "        self.num_copy = m\n",
    "        \n",
    "    def apply(self, nn_state, samples):\n",
    "        \"\"\"\n",
    "        This function calcualte <x1 x2 ... xm | rho^{\\otimes m} O | xm x1 x2 ... xm-1> / <x1 x2 ... xm | rho^{\\otimes m} | x1 x2 ... xm>\n",
    "        where O acts only on the first register.\n",
    "        \"\"\"\n",
    "        \n",
    "        # [num_sample, num_visible_node]\n",
    "        # samples = [s1, s2, s3 ... sN]\n",
    "        #  where num_sample = N, and si is num_visible_node-bits\n",
    "        samples = samples.to(device=nn_state.device)\n",
    "        \n",
    "        num_sample, num_visible_node = samples.shape\n",
    "        \n",
    "        # [num_sample, num_visible_node * num_copy]\n",
    "        # samples_array = [[s1 sN sN-1], [s2 s1 sN], [s3 s2 s1],.. [sN sN-1 sN-2]]\n",
    "        #  each row is num_copy*num_visible_node bits the above example is for num_copy=3\n",
    "        samples_array = []\n",
    "        for copy_index in range(self.num_copy):\n",
    "            rolled_samples = torch.roll(samples, shifts=copy_index, dims=0)\n",
    "            samples_array.append(rolled_samples)\n",
    "        samples_array = torch.hstack(samples_array)\n",
    "        assert(samples_array.shape[0] == num_sample)\n",
    "        assert(samples_array.shape[1] == num_visible_node * self.num_copy)\n",
    "        \n",
    "        # roll second dim of [num_sample, num_visible_node * num_copy] by num_visible_node\n",
    "        # swapped_samples_array = [[sN-1 s1 sN], [sN s2 s1], [s1 s3 s2],.. [sN-2 sN sN-1]]\n",
    "        swapped_samples_array = torch.roll(samples_array, shifts = num_visible_node, dims=1)\n",
    "\n",
    "        # pick copy of first block\n",
    "        #  first_block_sample = [sN-1, sN, s1, s2, ... sN-2]\n",
    "        first_block_sample = swapped_samples_array[:, :num_visible_node].clone()\n",
    "\n",
    "        # calculate coefficient for first block [num_samples, 0:num_visible_node]\n",
    "        total_prod = cplx.make_complex(torch.ones_like(samples[:,0]), torch.zeros_like(samples[:,0]))\n",
    "        for index, pauli in self.pauli_dict.items():\n",
    "            assert(index < num_visible_node)\n",
    "            coeff = to_pm1(first_block_sample[:, index])\n",
    "            if pauli == \"Z\":\n",
    "                coeff = cplx.make_complex(coeff, torch.zeros_like(coeff))\n",
    "                total_prod = cplx.elementwise_mult(coeff, total_prod)\n",
    "            elif pauli == \"Y\":\n",
    "                coeff = cplx.make_complex(torch.zeros_like(coeff), coeff)\n",
    "                total_prod = cplx.elementwise_mult(coeff, total_prod)\n",
    "        \n",
    "        # flip samples for for first block [num_samples, 0:num_visible_node]\n",
    "        # first_block_sample -> [OsN-1, OsN, Os1, Os2, ... OsN-2]\n",
    "        #  where Osi is bit array after Pauli bit-flips \n",
    "        for index, pauli in self.pauli_dict.items():\n",
    "            assert(index < num_visible_node)\n",
    "            if pauli in [\"X\", \"Y\"]:\n",
    "                first_block_sample = flip_spin(index, first_block_sample)\n",
    "\n",
    "\n",
    "        # store flipped first block\n",
    "        swapped_samples_array[:, :num_visible_node] = first_block_sample\n",
    "\n",
    "        # calculate product of coefficients\n",
    "        # samples_array = [[s1 sN sN-1], [s2 s1 sN], [s3 s2 s1],.. [sN sN-1 sN-2]]\n",
    "        # swapped_samples_array = [[OsN-1 s1 sN], [OsN s2 s1], [Os1 s3 s2],.. [OsN-2 sN sN-1]]\n",
    "        \"\"\"\n",
    "        total_prod = [\n",
    "            <s1 sN sN-1 | rho^{\\otimes 3} | OsN-1 s1 sN> / <s1 sN sN-1 | rho^{\\otimes 3} | s1 sN sN-1> , \n",
    "            <s2 s1 sN   | rho^{\\otimes 3} | OsN s2 s1>   / <s2 s1 sN   | rho^{\\otimes 3} | s2 s1 sN> , \n",
    "            <s3 s2 s1   | rho^{\\otimes 3} | Os1 s3 s2>   / <s3 s2 s1   | rho^{\\otimes 3} | s3 s2 s1> , \n",
    "\n",
    "        e.g. \n",
    "        <s3 s2 s1   | rho^{\\otimes 3} | Os1 s3 s2>   / <s3 s2 s1   | rho^{\\otimes 3} | s3 s2 s1>\n",
    "         = <s3 | rho | Os1> <s2 | rho | s3> < s1| rho | s2> / (<s3 | rho | s3> <s2 | rho | s2> < s1| rho | s1>)\n",
    "         =  (<s3 | rho | Os1> / <s3 | rho | s3>)\n",
    "          * (<s2 | rho | s3> / <s2 | rho | s2> )\n",
    "          * (< s1| rho | s2> / < s1| rho | s1>)\n",
    "         \n",
    "        importance_sampling_numerator(s3, Os1)  provides <s3 | rho | Os1>\n",
    "        importance_sampling_denominator(s3)     provides <s3 | rho | s3>\n",
    "        \"\"\"\n",
    "        for copy_index in range(self.num_copy):\n",
    "            st = copy_index * samples.shape[1]\n",
    "            en = (copy_index+1) * samples.shape[1]\n",
    "            # numerator is []\n",
    "            numerator = nn_state.importance_sampling_numerator(swapped_samples_array[:, st:en], samples_array[:, st:en])\n",
    "            denominator = nn_state.importance_sampling_denominator(samples_array[:, st:en])\n",
    "            values = cplx.elementwise_division(numerator, denominator)\n",
    "            total_prod = cplx.elementwise_mult(total_prod, values)\n",
    "\n",
    "        value = cplx.real(total_prod)\n",
    "        return value\n",
    "\n",
    "def calculate_distilled_expectation_value(pauli_dict: dict, num_samples: int, num_copies: int):\n",
    "    obs_num = GeneralPauliDistill(pauli_dict, num_copies)\n",
    "    obs_div = GeneralPauliDistill({}, num_copies)\n",
    "    num_stat = obs_num.statistics(nn_state_dm, num_samples=num_samples)\n",
    "    div_stat = obs_div.statistics(nn_state_dm, num_samples=num_samples)\n",
    "\n",
    "    from uncertainties import ufloat\n",
    "    num = ufloat(num_stat[\"mean\"], num_stat[\"std_error\"])\n",
    "    div = ufloat(div_stat[\"mean\"], div_stat[\"std_error\"])\n",
    "    val = num/div\n",
    "    result_dict = {\"mean\": val.n , \"std_error\": val.s, \"num_samples\": num_samples, \"num_copies\": num_copies}\n",
    "    return result_dict\n",
    "\n",
    "def get_density_matrix(nn_state):\n",
    "    space = nn_state.generate_hilbert_space()\n",
    "    Z = nn_state.normalization(space)\n",
    "    tensor = nn_state.rho(space, space)/Z\n",
    "    matrix = cplx.numpy(tensor)\n",
    "    return matrix\n",
    "\n",
    "def get_max_eigvec(matrix):\n",
    "    e_val, e_vec = np.linalg.eigh(matrix)\n",
    "    me_val = e_val[-1]\n",
    "    me_vec = e_vec[:,-1]\n",
    "    return me_vec\n",
    "\n",
    "def get_eigvec(nn_state, obs, space, **kwargs):\n",
    "    dm = get_density_matrix(nn_state)\n",
    "    ev = get_max_eigvec(dm)\n",
    "    ev = np.atleast_2d(ev)\n",
    "    val = ev@obs@ev.T.conj()\n",
    "    val = val[0,0].real\n",
    "    return val\n",
    "\n",
    "def mev_fidelity(nn_state, **kwargs):\n",
    "    ideal_state_vector = np.array([(1/np.sqrt(2)), 0, 0, (1/np.sqrt(2))])\n",
    "    matrix = get_density_matrix(nn_state)\n",
    "    mev = get_max_eigvec(matrix)/np.linalg.norm(get_max_eigvec(matrix))\n",
    "    fidelity = np.abs(mev@ideal_state_vector.T.conj())**2\n",
    "    return fidelity\n",
    "\n",
    "def observable_XX():\n",
    "    target_list = [0, 1]\n",
    "    pauli_index = [1, 1] # 1:X , 2:Y, 3:Z\n",
    "    gate = Pauli(target_list, pauli_index) # = X_1 X_2\n",
    "    return gate.get_matrix()\n",
    "\n",
    "def observable_XZ():\n",
    "    target_list = [0, 1]\n",
    "    pauli_index = [1, 3] # 1:X , 2:Y, 3:Z\n",
    "    gate = Pauli(target_list, pauli_index) # = X_1 Z_2\n",
    "    return gate.get_matrix()\n",
    "\n",
    "def observable_XX_ev(nn_state, **kwargs):\n",
    "    obs_stat = calculate_distilled_expectation_value({0: \"X\", 1: \"X\"}, n_sampling, n_copy)\n",
    "    return obs_stat[\"mean\"]\n",
    "\n",
    "def observable_XZ_ev(nn_state, **kwargs):\n",
    "    obs_stat = calculate_distilled_expectation_value({0: \"X\", 1: \"Z\"}, n_sampling, n_copy)\n",
    "    return obs_stat[\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b4095-4a33-4938-9e25-f9d6cce67c73",
   "metadata": {},
   "source": [
    "## callback setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6433a987-cec4-4698-8170-cf2955012fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callback_dm(nn_state):\n",
    "    metric_dict = {\n",
    "        \"Fidelity\": mev_fidelity,\n",
    "        \"KL_Divergence\": ts.KL,\n",
    "        #\"Observable_XX_ev\": observable_XX_ev,\n",
    "        #\"Observable_XZ_ev\": observable_XZ_ev,\n",
    "    }\n",
    "\n",
    "    space = nn_state.generate_hilbert_space()\n",
    "    callbacks = [\n",
    "        MetricEvaluator(\n",
    "            period,\n",
    "            metric_dict,\n",
    "            target = ideal_rho,\n",
    "            bases = meas_pattern,\n",
    "            verbose = True,\n",
    "            space = space,\n",
    "        )\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd8047-5824-410e-a1cb-1851fcfc3f35",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "990cd783-aada-493b-b806-c64e330c1c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_model : unitary, each_n_shot : 1000\n",
      "Epoch: 1\tFidelity = 0.208218\tKL_Divergence = 0.499330\n",
      "Epoch: 2\tFidelity = 0.239022\tKL_Divergence = 0.449647\n",
      "Epoch: 3\tFidelity = 0.297796\tKL_Divergence = 0.397582\n",
      "Epoch: 4\tFidelity = 0.407804\tKL_Divergence = 0.340470\n",
      "Epoch: 5\tFidelity = 0.503120\tKL_Divergence = 0.287224\n",
      "Epoch: 6\tFidelity = 0.607058\tKL_Divergence = 0.242548\n",
      "Epoch: 7\tFidelity = 0.705046\tKL_Divergence = 0.220966\n",
      "Epoch: 8\tFidelity = 0.798500\tKL_Divergence = 0.187685\n",
      "Epoch: 9\tFidelity = 0.843311\tKL_Divergence = 0.172913\n",
      "Epoch: 10\tFidelity = 0.910926\tKL_Divergence = 0.150975\n",
      "Epoch: 11\tFidelity = 0.937001\tKL_Divergence = 0.139013\n",
      "Epoch: 12\tFidelity = 0.929388\tKL_Divergence = 0.135851\n",
      "Epoch: 13\tFidelity = 0.939102\tKL_Divergence = 0.128176\n",
      "Epoch: 14\tFidelity = 0.968717\tKL_Divergence = 0.116077\n",
      "Epoch: 15\tFidelity = 0.979631\tKL_Divergence = 0.107287\n",
      "Epoch: 16\tFidelity = 0.960153\tKL_Divergence = 0.108760\n",
      "Epoch: 17\tFidelity = 0.986137\tKL_Divergence = 0.094626\n",
      "Epoch: 18\tFidelity = 0.968553\tKL_Divergence = 0.096588\n",
      "Epoch: 19\tFidelity = 0.980159\tKL_Divergence = 0.083962\n",
      "Epoch: 20\tFidelity = 0.970376\tKL_Divergence = 0.083980\n",
      "Epoch: 21\tFidelity = 0.994442\tKL_Divergence = 0.068299\n",
      "Epoch: 22\tFidelity = 0.973678\tKL_Divergence = 0.075061\n",
      "Epoch: 23\tFidelity = 0.992944\tKL_Divergence = 0.058813\n",
      "Epoch: 24\tFidelity = 0.974545\tKL_Divergence = 0.068229\n",
      "Epoch: 25\tFidelity = 0.990380\tKL_Divergence = 0.053111\n",
      "Epoch: 26\tFidelity = 0.986002\tKL_Divergence = 0.049996\n",
      "Epoch: 27\tFidelity = 0.994275\tKL_Divergence = 0.043307\n",
      "Epoch: 28\tFidelity = 0.996910\tKL_Divergence = 0.038415\n",
      "Epoch: 29\tFidelity = 0.975596\tKL_Divergence = 0.051025\n",
      "Epoch: 30\tFidelity = 0.981097\tKL_Divergence = 0.048112\n",
      "Epoch: 31\tFidelity = 0.996566\tKL_Divergence = 0.031769\n",
      "Epoch: 32\tFidelity = 0.988046\tKL_Divergence = 0.038357\n",
      "Epoch: 33\tFidelity = 0.998807\tKL_Divergence = 0.026389\n",
      "Epoch: 34\tFidelity = 0.997990\tKL_Divergence = 0.025318\n",
      "Epoch: 35\tFidelity = 0.990048\tKL_Divergence = 0.032347\n",
      "Epoch: 36\tFidelity = 0.997881\tKL_Divergence = 0.022730\n",
      "Epoch: 37\tFidelity = 0.994985\tKL_Divergence = 0.023962\n",
      "Epoch: 38\tFidelity = 0.993690\tKL_Divergence = 0.025645\n",
      "Epoch: 39\tFidelity = 0.997776\tKL_Divergence = 0.020254\n",
      "Epoch: 40\tFidelity = 0.997116\tKL_Divergence = 0.020258\n",
      "Epoch: 41\tFidelity = 0.995244\tKL_Divergence = 0.019990\n",
      "Epoch: 42\tFidelity = 0.998477\tKL_Divergence = 0.017345\n",
      "Epoch: 43\tFidelity = 0.993922\tKL_Divergence = 0.018912\n",
      "Epoch: 44\tFidelity = 0.990352\tKL_Divergence = 0.024017\n",
      "Epoch: 45\tFidelity = 0.996577\tKL_Divergence = 0.016716\n",
      "Epoch: 46\tFidelity = 0.993493\tKL_Divergence = 0.018204\n",
      "Epoch: 47\tFidelity = 0.998629\tKL_Divergence = 0.013949\n",
      "Epoch: 48\tFidelity = 0.996234\tKL_Divergence = 0.014966\n",
      "Epoch: 49\tFidelity = 0.991135\tKL_Divergence = 0.022139\n",
      "Epoch: 50\tFidelity = 0.997442\tKL_Divergence = 0.014417\n",
      "Epoch: 51\tFidelity = 0.995006\tKL_Divergence = 0.016256\n",
      "Epoch: 52\tFidelity = 0.999237\tKL_Divergence = 0.011928\n",
      "Epoch: 53\tFidelity = 0.993972\tKL_Divergence = 0.017713\n",
      "Epoch: 54\tFidelity = 0.998760\tKL_Divergence = 0.011728\n",
      "Epoch: 55\tFidelity = 0.988042\tKL_Divergence = 0.023921\n",
      "Epoch: 56\tFidelity = 0.992822\tKL_Divergence = 0.015584\n",
      "Epoch: 57\tFidelity = 0.997365\tKL_Divergence = 0.012657\n",
      "Epoch: 58\tFidelity = 0.999057\tKL_Divergence = 0.010586\n",
      "Epoch: 59\tFidelity = 0.997180\tKL_Divergence = 0.011368\n",
      "Epoch: 60\tFidelity = 0.997437\tKL_Divergence = 0.011141\n",
      "Epoch: 61\tFidelity = 0.998567\tKL_Divergence = 0.010257\n",
      "Epoch: 62\tFidelity = 0.996764\tKL_Divergence = 0.012773\n",
      "Epoch: 63\tFidelity = 0.999064\tKL_Divergence = 0.009404\n",
      "Epoch: 64\tFidelity = 0.994708\tKL_Divergence = 0.014833\n",
      "Epoch: 65\tFidelity = 0.999130\tKL_Divergence = 0.009364\n",
      "Epoch: 66\tFidelity = 0.997383\tKL_Divergence = 0.010948\n",
      "Epoch: 67\tFidelity = 0.998882\tKL_Divergence = 0.008925\n",
      "Epoch: 68\tFidelity = 0.996862\tKL_Divergence = 0.011475\n",
      "Epoch: 69\tFidelity = 0.991428\tKL_Divergence = 0.017656\n",
      "Epoch: 70\tFidelity = 0.998368\tKL_Divergence = 0.009290\n",
      "Epoch: 71\tFidelity = 0.999396\tKL_Divergence = 0.008052\n",
      "Epoch: 72\tFidelity = 0.999475\tKL_Divergence = 0.007696\n",
      "Epoch: 73\tFidelity = 0.999059\tKL_Divergence = 0.007834\n",
      "Epoch: 74\tFidelity = 0.994457\tKL_Divergence = 0.013004\n",
      "Epoch: 75\tFidelity = 0.999419\tKL_Divergence = 0.007672\n",
      "Epoch: 76\tFidelity = 0.997066\tKL_Divergence = 0.009701\n",
      "Epoch: 77\tFidelity = 0.999642\tKL_Divergence = 0.007068\n",
      "Epoch: 78\tFidelity = 0.999115\tKL_Divergence = 0.007362\n",
      "Epoch: 79\tFidelity = 0.997019\tKL_Divergence = 0.010330\n",
      "Epoch: 80\tFidelity = 0.999675\tKL_Divergence = 0.006818\n",
      "Epoch: 81\tFidelity = 0.999251\tKL_Divergence = 0.007109\n",
      "Epoch: 82\tFidelity = 0.998682\tKL_Divergence = 0.007905\n",
      "Epoch: 83\tFidelity = 0.994446\tKL_Divergence = 0.012418\n",
      "Epoch: 84\tFidelity = 0.996456\tKL_Divergence = 0.010187\n",
      "Epoch: 85\tFidelity = 0.999735\tKL_Divergence = 0.006449\n",
      "Epoch: 86\tFidelity = 0.998500\tKL_Divergence = 0.007147\n",
      "Epoch: 87\tFidelity = 0.998316\tKL_Divergence = 0.007603\n",
      "Epoch: 88\tFidelity = 0.997109\tKL_Divergence = 0.009394\n",
      "Epoch: 89\tFidelity = 0.998816\tKL_Divergence = 0.006812\n",
      "Epoch: 90\tFidelity = 0.997327\tKL_Divergence = 0.007739\n",
      "Epoch: 91\tFidelity = 0.999611\tKL_Divergence = 0.006091\n",
      "Epoch: 92\tFidelity = 0.997384\tKL_Divergence = 0.008569\n",
      "Epoch: 93\tFidelity = 0.998732\tKL_Divergence = 0.007231\n",
      "Epoch: 94\tFidelity = 0.996744\tKL_Divergence = 0.009471\n",
      "Epoch: 95\tFidelity = 0.998086\tKL_Divergence = 0.006848\n",
      "Epoch: 96\tFidelity = 0.997732\tKL_Divergence = 0.008049\n",
      "Epoch: 97\tFidelity = 0.998415\tKL_Divergence = 0.007329\n",
      "Epoch: 98\tFidelity = 0.996695\tKL_Divergence = 0.009586\n",
      "Epoch: 99\tFidelity = 0.999527\tKL_Divergence = 0.005899\n",
      "Epoch: 100\tFidelity = 0.999452\tKL_Divergence = 0.005789\n",
      "Epoch: 101\tFidelity = 0.997313\tKL_Divergence = 0.008530\n",
      "Epoch: 102\tFidelity = 0.997781\tKL_Divergence = 0.007920\n",
      "Epoch: 103\tFidelity = 0.999594\tKL_Divergence = 0.005502\n",
      "Epoch: 104\tFidelity = 0.998810\tKL_Divergence = 0.005971\n",
      "Epoch: 105\tFidelity = 0.999747\tKL_Divergence = 0.005342\n",
      "Epoch: 106\tFidelity = 0.999457\tKL_Divergence = 0.005504\n",
      "Epoch: 107\tFidelity = 0.996581\tKL_Divergence = 0.009387\n",
      "Epoch: 108\tFidelity = 0.997441\tKL_Divergence = 0.007909\n",
      "Epoch: 109\tFidelity = 0.999317\tKL_Divergence = 0.005499\n",
      "Epoch: 110\tFidelity = 0.998387\tKL_Divergence = 0.006112\n",
      "Epoch: 111\tFidelity = 0.999499\tKL_Divergence = 0.005289\n",
      "Epoch: 112\tFidelity = 0.999082\tKL_Divergence = 0.005931\n",
      "Epoch: 113\tFidelity = 0.999704\tKL_Divergence = 0.005162\n",
      "Epoch: 114\tFidelity = 0.997820\tKL_Divergence = 0.007578\n",
      "Epoch: 115\tFidelity = 0.999080\tKL_Divergence = 0.005929\n",
      "Epoch: 116\tFidelity = 0.999694\tKL_Divergence = 0.005158\n",
      "Epoch: 117\tFidelity = 0.999637\tKL_Divergence = 0.005138\n",
      "Epoch: 118\tFidelity = 0.999221\tKL_Divergence = 0.005512\n",
      "Epoch: 119\tFidelity = 0.999164\tKL_Divergence = 0.005657\n",
      "Epoch: 120\tFidelity = 0.998012\tKL_Divergence = 0.006060\n",
      "Epoch: 121\tFidelity = 0.999497\tKL_Divergence = 0.005138\n",
      "Epoch: 122\tFidelity = 0.998305\tKL_Divergence = 0.006051\n",
      "Epoch: 123\tFidelity = 0.999676\tKL_Divergence = 0.004990\n",
      "Epoch: 124\tFidelity = 0.999493\tKL_Divergence = 0.005035\n",
      "Epoch: 125\tFidelity = 0.997946\tKL_Divergence = 0.007215\n",
      "Epoch: 126\tFidelity = 0.998475\tKL_Divergence = 0.006401\n",
      "Epoch: 127\tFidelity = 0.998345\tKL_Divergence = 0.006093\n",
      "Epoch: 128\tFidelity = 0.998059\tKL_Divergence = 0.006724\n",
      "Epoch: 129\tFidelity = 0.998828\tKL_Divergence = 0.005885\n",
      "Epoch: 130\tFidelity = 0.999458\tKL_Divergence = 0.005044\n",
      "Epoch: 131\tFidelity = 0.999104\tKL_Divergence = 0.005411\n",
      "Epoch: 132\tFidelity = 0.999792\tKL_Divergence = 0.004612\n",
      "Epoch: 133\tFidelity = 0.997395\tKL_Divergence = 0.007552\n",
      "Epoch: 134\tFidelity = 0.998704\tKL_Divergence = 0.005932\n",
      "Epoch: 135\tFidelity = 0.991875\tKL_Divergence = 0.014686\n",
      "Epoch: 136\tFidelity = 0.996999\tKL_Divergence = 0.007918\n",
      "Epoch: 137\tFidelity = 0.999055\tKL_Divergence = 0.005477\n",
      "Epoch: 138\tFidelity = 0.999443\tKL_Divergence = 0.004817\n",
      "Epoch: 139\tFidelity = 0.999286\tKL_Divergence = 0.004928\n",
      "Epoch: 140\tFidelity = 0.999154\tKL_Divergence = 0.005141\n",
      "Epoch: 141\tFidelity = 0.994466\tKL_Divergence = 0.011113\n",
      "Epoch: 142\tFidelity = 0.998666\tKL_Divergence = 0.005883\n",
      "Epoch: 143\tFidelity = 0.999635\tKL_Divergence = 0.004601\n",
      "Epoch: 144\tFidelity = 0.997859\tKL_Divergence = 0.006622\n",
      "Epoch: 145\tFidelity = 0.999529\tKL_Divergence = 0.004523\n",
      "Epoch: 146\tFidelity = 0.997404\tKL_Divergence = 0.007342\n",
      "Epoch: 147\tFidelity = 0.999512\tKL_Divergence = 0.004668\n",
      "Epoch: 148\tFidelity = 0.998542\tKL_Divergence = 0.005541\n",
      "Epoch: 149\tFidelity = 0.993500\tKL_Divergence = 0.012342\n",
      "Epoch: 150\tFidelity = 0.998587\tKL_Divergence = 0.005783\n",
      "Epoch: 151\tFidelity = 0.999758\tKL_Divergence = 0.004305\n",
      "Epoch: 152\tFidelity = 0.996532\tKL_Divergence = 0.008411\n",
      "Epoch: 153\tFidelity = 0.998417\tKL_Divergence = 0.005953\n",
      "Epoch: 154\tFidelity = 0.999797\tKL_Divergence = 0.004256\n",
      "Epoch: 155\tFidelity = 0.999697\tKL_Divergence = 0.004333\n",
      "Epoch: 156\tFidelity = 0.996295\tKL_Divergence = 0.008692\n",
      "Epoch: 157\tFidelity = 0.996665\tKL_Divergence = 0.007965\n",
      "Epoch: 158\tFidelity = 0.998394\tKL_Divergence = 0.005980\n",
      "Epoch: 159\tFidelity = 0.999101\tKL_Divergence = 0.005014\n",
      "Epoch: 160\tFidelity = 0.998962\tKL_Divergence = 0.005087\n",
      "Epoch: 161\tFidelity = 0.998900\tKL_Divergence = 0.005311\n",
      "Epoch: 162\tFidelity = 0.999250\tKL_Divergence = 0.004836\n",
      "Epoch: 163\tFidelity = 0.999689\tKL_Divergence = 0.004288\n",
      "Epoch: 164\tFidelity = 0.998996\tKL_Divergence = 0.004955\n",
      "Epoch: 165\tFidelity = 0.995717\tKL_Divergence = 0.009362\n",
      "Epoch: 166\tFidelity = 0.996799\tKL_Divergence = 0.007886\n",
      "Epoch: 167\tFidelity = 0.999377\tKL_Divergence = 0.004611\n",
      "Epoch: 168\tFidelity = 0.999387\tKL_Divergence = 0.004546\n",
      "Epoch: 169\tFidelity = 0.998360\tKL_Divergence = 0.005906\n",
      "Epoch: 170\tFidelity = 0.999198\tKL_Divergence = 0.004586\n",
      "Epoch: 171\tFidelity = 0.994295\tKL_Divergence = 0.011037\n",
      "Epoch: 172\tFidelity = 0.998906\tKL_Divergence = 0.005188\n",
      "Epoch: 173\tFidelity = 0.998960\tKL_Divergence = 0.005068\n",
      "Epoch: 174\tFidelity = 0.997849\tKL_Divergence = 0.006471\n",
      "Epoch: 175\tFidelity = 0.998299\tKL_Divergence = 0.005772\n",
      "Epoch: 176\tFidelity = 0.998872\tKL_Divergence = 0.005009\n",
      "Epoch: 177\tFidelity = 0.998175\tKL_Divergence = 0.005992\n",
      "Epoch: 178\tFidelity = 0.998947\tKL_Divergence = 0.004831\n",
      "Epoch: 179\tFidelity = 0.994524\tKL_Divergence = 0.010668\n",
      "Epoch: 180\tFidelity = 0.998645\tKL_Divergence = 0.005478\n",
      "Epoch: 181\tFidelity = 0.996548\tKL_Divergence = 0.008122\n",
      "Epoch: 182\tFidelity = 0.999124\tKL_Divergence = 0.004773\n",
      "Epoch: 183\tFidelity = 0.997195\tKL_Divergence = 0.007097\n",
      "Epoch: 184\tFidelity = 0.996329\tKL_Divergence = 0.008322\n",
      "Epoch: 185\tFidelity = 0.997716\tKL_Divergence = 0.006604\n",
      "Epoch: 186\tFidelity = 0.999505\tKL_Divergence = 0.004338\n",
      "Epoch: 187\tFidelity = 0.998043\tKL_Divergence = 0.006092\n",
      "Epoch: 188\tFidelity = 0.996463\tKL_Divergence = 0.008127\n",
      "Epoch: 189\tFidelity = 0.996810\tKL_Divergence = 0.007752\n",
      "Epoch: 190\tFidelity = 0.996876\tKL_Divergence = 0.007664\n",
      "Epoch: 191\tFidelity = 0.996741\tKL_Divergence = 0.007665\n",
      "Epoch: 192\tFidelity = 0.999001\tKL_Divergence = 0.004917\n",
      "Epoch: 193\tFidelity = 0.999053\tKL_Divergence = 0.004730\n",
      "Epoch: 194\tFidelity = 0.997259\tKL_Divergence = 0.007069\n",
      "Epoch: 195\tFidelity = 0.999069\tKL_Divergence = 0.004671\n",
      "Epoch: 196\tFidelity = 0.999544\tKL_Divergence = 0.004187\n",
      "Epoch: 197\tFidelity = 0.999435\tKL_Divergence = 0.004335\n",
      "Epoch: 198\tFidelity = 0.999628\tKL_Divergence = 0.004086\n",
      "Epoch: 199\tFidelity = 0.999700\tKL_Divergence = 0.004004\n",
      "Epoch: 200\tFidelity = 0.999438\tKL_Divergence = 0.004159\n",
      "Epoch: 201\tFidelity = 0.997116\tKL_Divergence = 0.007189\n",
      "Epoch: 202\tFidelity = 0.994003\tKL_Divergence = 0.011326\n",
      "Epoch: 203\tFidelity = 0.991983\tKL_Divergence = 0.013803\n",
      "Epoch: 204\tFidelity = 0.998021\tKL_Divergence = 0.006105\n",
      "Epoch: 205\tFidelity = 0.999531\tKL_Divergence = 0.004187\n",
      "Epoch: 206\tFidelity = 0.998940\tKL_Divergence = 0.004921\n",
      "Epoch: 207\tFidelity = 0.999555\tKL_Divergence = 0.004145\n",
      "Epoch: 208\tFidelity = 0.998442\tKL_Divergence = 0.005566\n",
      "Epoch: 209\tFidelity = 0.997311\tKL_Divergence = 0.006987\n",
      "Epoch: 210\tFidelity = 0.999583\tKL_Divergence = 0.004103\n",
      "Epoch: 211\tFidelity = 0.999170\tKL_Divergence = 0.004583\n",
      "Epoch: 212\tFidelity = 0.999055\tKL_Divergence = 0.004483\n",
      "Epoch: 213\tFidelity = 0.999463\tKL_Divergence = 0.004233\n",
      "Epoch: 214\tFidelity = 0.999141\tKL_Divergence = 0.004536\n",
      "Epoch: 215\tFidelity = 0.999579\tKL_Divergence = 0.003968\n",
      "Epoch: 216\tFidelity = 0.998771\tKL_Divergence = 0.005011\n",
      "Epoch: 217\tFidelity = 0.997924\tKL_Divergence = 0.006179\n",
      "Epoch: 218\tFidelity = 0.998497\tKL_Divergence = 0.005430\n",
      "Epoch: 219\tFidelity = 0.997517\tKL_Divergence = 0.006624\n",
      "Epoch: 220\tFidelity = 0.999557\tKL_Divergence = 0.004022\n",
      "Epoch: 221\tFidelity = 0.996798\tKL_Divergence = 0.007556\n",
      "Epoch: 222\tFidelity = 0.998581\tKL_Divergence = 0.005290\n",
      "Epoch: 223\tFidelity = 0.998106\tKL_Divergence = 0.005842\n",
      "Epoch: 224\tFidelity = 0.997156\tKL_Divergence = 0.007112\n",
      "Epoch: 225\tFidelity = 0.997854\tKL_Divergence = 0.006200\n",
      "Epoch: 226\tFidelity = 0.998669\tKL_Divergence = 0.005144\n",
      "Epoch: 227\tFidelity = 0.996159\tKL_Divergence = 0.008401\n",
      "Epoch: 228\tFidelity = 0.997106\tKL_Divergence = 0.007164\n",
      "Epoch: 229\tFidelity = 0.998132\tKL_Divergence = 0.005672\n",
      "Epoch: 230\tFidelity = 0.999462\tKL_Divergence = 0.004144\n",
      "Epoch: 231\tFidelity = 0.999762\tKL_Divergence = 0.003772\n",
      "Epoch: 232\tFidelity = 0.999595\tKL_Divergence = 0.003876\n",
      "Epoch: 233\tFidelity = 0.999368\tKL_Divergence = 0.004249\n",
      "Epoch: 234\tFidelity = 0.998503\tKL_Divergence = 0.005345\n",
      "Epoch: 235\tFidelity = 0.998994\tKL_Divergence = 0.004584\n",
      "Epoch: 236\tFidelity = 0.998173\tKL_Divergence = 0.005744\n",
      "Epoch: 237\tFidelity = 0.999538\tKL_Divergence = 0.003893\n",
      "Epoch: 238\tFidelity = 0.999526\tKL_Divergence = 0.003967\n",
      "Epoch: 239\tFidelity = 0.999360\tKL_Divergence = 0.004103\n",
      "Epoch: 240\tFidelity = 0.999737\tKL_Divergence = 0.003747\n",
      "Epoch: 241\tFidelity = 0.999340\tKL_Divergence = 0.004201\n",
      "Epoch: 242\tFidelity = 0.998675\tKL_Divergence = 0.004984\n",
      "Epoch: 243\tFidelity = 0.998974\tKL_Divergence = 0.004702\n",
      "Epoch: 244\tFidelity = 0.997705\tKL_Divergence = 0.006310\n",
      "Epoch: 245\tFidelity = 0.997313\tKL_Divergence = 0.006841\n",
      "Epoch: 246\tFidelity = 0.998089\tKL_Divergence = 0.005843\n",
      "Epoch: 247\tFidelity = 0.998938\tKL_Divergence = 0.004727\n",
      "Epoch: 248\tFidelity = 0.999262\tKL_Divergence = 0.004324\n",
      "Epoch: 249\tFidelity = 0.999184\tKL_Divergence = 0.004440\n",
      "Epoch: 250\tFidelity = 0.998612\tKL_Divergence = 0.005150\n",
      "Epoch: 251\tFidelity = 0.997301\tKL_Divergence = 0.006800\n",
      "Epoch: 252\tFidelity = 0.998323\tKL_Divergence = 0.005516\n",
      "Epoch: 253\tFidelity = 0.999336\tKL_Divergence = 0.004218\n",
      "Epoch: 254\tFidelity = 0.999431\tKL_Divergence = 0.004103\n",
      "Epoch: 255\tFidelity = 0.998312\tKL_Divergence = 0.005502\n",
      "Epoch: 256\tFidelity = 0.996695\tKL_Divergence = 0.007569\n",
      "Epoch: 257\tFidelity = 0.996774\tKL_Divergence = 0.007480\n",
      "Epoch: 258\tFidelity = 0.998711\tKL_Divergence = 0.004967\n",
      "Epoch: 259\tFidelity = 0.998011\tKL_Divergence = 0.005904\n",
      "Epoch: 260\tFidelity = 0.998814\tKL_Divergence = 0.004854\n",
      "Epoch: 261\tFidelity = 0.998757\tKL_Divergence = 0.004898\n",
      "Epoch: 262\tFidelity = 0.999075\tKL_Divergence = 0.004506\n",
      "Epoch: 263\tFidelity = 0.999491\tKL_Divergence = 0.003983\n",
      "Epoch: 264\tFidelity = 0.997988\tKL_Divergence = 0.005931\n",
      "Epoch: 265\tFidelity = 0.997550\tKL_Divergence = 0.006478\n",
      "Epoch: 266\tFidelity = 0.996874\tKL_Divergence = 0.007357\n",
      "Epoch: 267\tFidelity = 0.998124\tKL_Divergence = 0.005724\n",
      "Epoch: 268\tFidelity = 0.997490\tKL_Divergence = 0.006553\n",
      "Epoch: 269\tFidelity = 0.996206\tKL_Divergence = 0.008194\n",
      "Epoch: 270\tFidelity = 0.998542\tKL_Divergence = 0.005193\n",
      "Epoch: 271\tFidelity = 0.998587\tKL_Divergence = 0.005127\n",
      "Epoch: 272\tFidelity = 0.997558\tKL_Divergence = 0.006420\n",
      "Epoch: 273\tFidelity = 0.998258\tKL_Divergence = 0.005502\n",
      "Epoch: 274\tFidelity = 0.996963\tKL_Divergence = 0.007214\n",
      "Epoch: 275\tFidelity = 0.997647\tKL_Divergence = 0.006339\n",
      "Epoch: 276\tFidelity = 0.998799\tKL_Divergence = 0.004849\n",
      "Epoch: 277\tFidelity = 0.998289\tKL_Divergence = 0.005501\n",
      "Epoch: 278\tFidelity = 0.997784\tKL_Divergence = 0.006139\n",
      "Epoch: 279\tFidelity = 0.999690\tKL_Divergence = 0.003700\n",
      "Epoch: 280\tFidelity = 0.999724\tKL_Divergence = 0.003665\n",
      "Epoch: 281\tFidelity = 0.999306\tKL_Divergence = 0.004178\n",
      "Epoch: 282\tFidelity = 0.997717\tKL_Divergence = 0.006156\n",
      "Epoch: 283\tFidelity = 0.997494\tKL_Divergence = 0.006516\n",
      "Epoch: 284\tFidelity = 0.997122\tKL_Divergence = 0.006998\n",
      "Epoch: 285\tFidelity = 0.996745\tKL_Divergence = 0.007454\n",
      "Epoch: 286\tFidelity = 0.995876\tKL_Divergence = 0.008589\n",
      "Epoch: 287\tFidelity = 0.996828\tKL_Divergence = 0.007364\n",
      "Epoch: 288\tFidelity = 0.997082\tKL_Divergence = 0.007036\n",
      "Epoch: 289\tFidelity = 0.998159\tKL_Divergence = 0.005646\n",
      "Epoch: 290\tFidelity = 0.999149\tKL_Divergence = 0.004363\n",
      "Epoch: 291\tFidelity = 0.998201\tKL_Divergence = 0.005592\n",
      "Epoch: 292\tFidelity = 0.997082\tKL_Divergence = 0.007023\n",
      "Epoch: 293\tFidelity = 0.997529\tKL_Divergence = 0.006444\n",
      "Epoch: 294\tFidelity = 0.996764\tKL_Divergence = 0.007427\n",
      "Epoch: 295\tFidelity = 0.996424\tKL_Divergence = 0.007818\n",
      "Epoch: 296\tFidelity = 0.997476\tKL_Divergence = 0.006499\n",
      "Epoch: 297\tFidelity = 0.998030\tKL_Divergence = 0.005784\n",
      "Epoch: 298\tFidelity = 0.997922\tKL_Divergence = 0.005925\n",
      "Epoch: 299\tFidelity = 0.998071\tKL_Divergence = 0.005730\n",
      "Epoch: 300\tFidelity = 0.998753\tKL_Divergence = 0.004844\n",
      "Epoch: 301\tFidelity = 0.998304\tKL_Divergence = 0.005429\n",
      "Epoch: 302\tFidelity = 0.998282\tKL_Divergence = 0.005454\n",
      "Epoch: 303\tFidelity = 0.997592\tKL_Divergence = 0.006335\n",
      "Epoch: 304\tFidelity = 0.995805\tKL_Divergence = 0.008652\n",
      "Epoch: 305\tFidelity = 0.997696\tKL_Divergence = 0.006203\n",
      "Epoch: 306\tFidelity = 0.996465\tKL_Divergence = 0.007792\n",
      "Epoch: 307\tFidelity = 0.997566\tKL_Divergence = 0.006371\n",
      "Epoch: 308\tFidelity = 0.998557\tKL_Divergence = 0.005091\n",
      "Epoch: 309\tFidelity = 0.998371\tKL_Divergence = 0.005326\n",
      "Epoch: 310\tFidelity = 0.998955\tKL_Divergence = 0.004567\n",
      "Epoch: 311\tFidelity = 0.998677\tKL_Divergence = 0.004934\n",
      "Epoch: 312\tFidelity = 0.998340\tKL_Divergence = 0.005365\n",
      "Epoch: 313\tFidelity = 0.997297\tKL_Divergence = 0.006703\n",
      "Epoch: 314\tFidelity = 0.997098\tKL_Divergence = 0.006958\n",
      "Epoch: 315\tFidelity = 0.995567\tKL_Divergence = 0.008937\n",
      "Epoch: 316\tFidelity = 0.996487\tKL_Divergence = 0.007743\n",
      "Epoch: 317\tFidelity = 0.997641\tKL_Divergence = 0.006220\n",
      "Epoch: 318\tFidelity = 0.997335\tKL_Divergence = 0.006647\n",
      "Epoch: 319\tFidelity = 0.998919\tKL_Divergence = 0.004598\n",
      "Epoch: 320\tFidelity = 0.999310\tKL_Divergence = 0.004109\n",
      "Epoch: 321\tFidelity = 0.998379\tKL_Divergence = 0.005286\n",
      "Epoch: 322\tFidelity = 0.999099\tKL_Divergence = 0.004365\n",
      "Epoch: 323\tFidelity = 0.998646\tKL_Divergence = 0.004949\n",
      "Epoch: 324\tFidelity = 0.999205\tKL_Divergence = 0.004220\n",
      "Epoch: 325\tFidelity = 0.998297\tKL_Divergence = 0.005394\n",
      "Epoch: 326\tFidelity = 0.997929\tKL_Divergence = 0.005874\n",
      "Epoch: 327\tFidelity = 0.997432\tKL_Divergence = 0.006511\n",
      "Epoch: 328\tFidelity = 0.996879\tKL_Divergence = 0.007225\n",
      "Epoch: 329\tFidelity = 0.998047\tKL_Divergence = 0.005714\n",
      "Epoch: 330\tFidelity = 0.997798\tKL_Divergence = 0.006039\n",
      "Epoch: 331\tFidelity = 0.997461\tKL_Divergence = 0.006470\n",
      "Epoch: 332\tFidelity = 0.998332\tKL_Divergence = 0.005346\n",
      "Epoch: 333\tFidelity = 0.996925\tKL_Divergence = 0.007152\n",
      "Epoch: 334\tFidelity = 0.996420\tKL_Divergence = 0.007786\n",
      "Epoch: 335\tFidelity = 0.997592\tKL_Divergence = 0.006285\n",
      "Epoch: 336\tFidelity = 0.996562\tKL_Divergence = 0.007619\n",
      "Epoch: 337\tFidelity = 0.997407\tKL_Divergence = 0.006531\n",
      "Epoch: 338\tFidelity = 0.997756\tKL_Divergence = 0.006077\n",
      "Epoch: 339\tFidelity = 0.998177\tKL_Divergence = 0.005515\n",
      "Epoch: 340\tFidelity = 0.998248\tKL_Divergence = 0.005440\n",
      "Epoch: 341\tFidelity = 0.997675\tKL_Divergence = 0.006171\n",
      "Epoch: 342\tFidelity = 0.997773\tKL_Divergence = 0.006044\n",
      "Epoch: 343\tFidelity = 0.998034\tKL_Divergence = 0.005701\n",
      "Epoch: 344\tFidelity = 0.997296\tKL_Divergence = 0.006650\n",
      "Epoch: 345\tFidelity = 0.997516\tKL_Divergence = 0.006382\n",
      "Epoch: 346\tFidelity = 0.998332\tKL_Divergence = 0.005328\n",
      "Epoch: 347\tFidelity = 0.997423\tKL_Divergence = 0.006503\n",
      "Epoch: 348\tFidelity = 0.997293\tKL_Divergence = 0.006670\n",
      "Epoch: 349\tFidelity = 0.997022\tKL_Divergence = 0.007016\n",
      "Epoch: 350\tFidelity = 0.997009\tKL_Divergence = 0.007030\n",
      "Epoch: 351\tFidelity = 0.997630\tKL_Divergence = 0.006235\n",
      "Epoch: 352\tFidelity = 0.997526\tKL_Divergence = 0.006365\n",
      "Epoch: 353\tFidelity = 0.997616\tKL_Divergence = 0.006247\n",
      "Epoch: 354\tFidelity = 0.997524\tKL_Divergence = 0.006357\n",
      "Epoch: 355\tFidelity = 0.998195\tKL_Divergence = 0.005491\n",
      "Epoch: 356\tFidelity = 0.997963\tKL_Divergence = 0.005790\n",
      "Epoch: 357\tFidelity = 0.998577\tKL_Divergence = 0.004997\n",
      "Epoch: 358\tFidelity = 0.998608\tKL_Divergence = 0.004951\n",
      "Epoch: 359\tFidelity = 0.998525\tKL_Divergence = 0.005068\n",
      "Epoch: 360\tFidelity = 0.998383\tKL_Divergence = 0.005255\n",
      "Epoch: 361\tFidelity = 0.997964\tKL_Divergence = 0.005792\n",
      "Epoch: 362\tFidelity = 0.998146\tKL_Divergence = 0.005552\n",
      "Epoch: 363\tFidelity = 0.998399\tKL_Divergence = 0.005222\n",
      "Epoch: 364\tFidelity = 0.997729\tKL_Divergence = 0.006072\n",
      "Epoch: 365\tFidelity = 0.997118\tKL_Divergence = 0.006877\n",
      "Epoch: 366\tFidelity = 0.997382\tKL_Divergence = 0.006538\n",
      "Epoch: 367\tFidelity = 0.997869\tKL_Divergence = 0.005908\n",
      "Epoch: 368\tFidelity = 0.998144\tKL_Divergence = 0.005556\n",
      "Epoch: 369\tFidelity = 0.998436\tKL_Divergence = 0.005184\n",
      "Epoch: 370\tFidelity = 0.997747\tKL_Divergence = 0.006070\n",
      "Epoch: 371\tFidelity = 0.997345\tKL_Divergence = 0.006590\n",
      "Epoch: 372\tFidelity = 0.997280\tKL_Divergence = 0.006674\n",
      "Epoch: 373\tFidelity = 0.997115\tKL_Divergence = 0.006884\n",
      "Epoch: 374\tFidelity = 0.997349\tKL_Divergence = 0.006581\n",
      "Epoch: 375\tFidelity = 0.996872\tKL_Divergence = 0.007195\n",
      "Epoch: 376\tFidelity = 0.996615\tKL_Divergence = 0.007527\n",
      "Epoch: 377\tFidelity = 0.997545\tKL_Divergence = 0.006322\n",
      "Epoch: 378\tFidelity = 0.997876\tKL_Divergence = 0.005897\n",
      "Epoch: 379\tFidelity = 0.997904\tKL_Divergence = 0.005862\n",
      "Epoch: 380\tFidelity = 0.997408\tKL_Divergence = 0.006500\n",
      "Epoch: 381\tFidelity = 0.997496\tKL_Divergence = 0.006383\n",
      "Epoch: 382\tFidelity = 0.998286\tKL_Divergence = 0.005365\n",
      "Epoch: 383\tFidelity = 0.998239\tKL_Divergence = 0.005421\n",
      "Epoch: 384\tFidelity = 0.998214\tKL_Divergence = 0.005458\n",
      "Epoch: 385\tFidelity = 0.998340\tKL_Divergence = 0.005295\n",
      "Epoch: 386\tFidelity = 0.998425\tKL_Divergence = 0.005189\n",
      "Epoch: 387\tFidelity = 0.997966\tKL_Divergence = 0.005771\n",
      "Epoch: 388\tFidelity = 0.997645\tKL_Divergence = 0.006188\n",
      "Epoch: 389\tFidelity = 0.997326\tKL_Divergence = 0.006602\n",
      "Epoch: 390\tFidelity = 0.997304\tKL_Divergence = 0.006625\n",
      "Epoch: 391\tFidelity = 0.997003\tKL_Divergence = 0.007013\n",
      "Epoch: 392\tFidelity = 0.997676\tKL_Divergence = 0.006143\n",
      "Epoch: 393\tFidelity = 0.998266\tKL_Divergence = 0.005388\n",
      "Epoch: 394\tFidelity = 0.997704\tKL_Divergence = 0.006108\n",
      "Epoch: 395\tFidelity = 0.997700\tKL_Divergence = 0.006110\n",
      "Epoch: 396\tFidelity = 0.997813\tKL_Divergence = 0.005966\n",
      "Epoch: 397\tFidelity = 0.997673\tKL_Divergence = 0.006145\n",
      "Epoch: 398\tFidelity = 0.997505\tKL_Divergence = 0.006360\n",
      "Epoch: 399\tFidelity = 0.997150\tKL_Divergence = 0.006823\n",
      "Epoch: 400\tFidelity = 0.997867\tKL_Divergence = 0.005895\n",
      "Epoch: 401\tFidelity = 0.997918\tKL_Divergence = 0.005828\n",
      "Epoch: 402\tFidelity = 0.997744\tKL_Divergence = 0.006053\n",
      "Epoch: 403\tFidelity = 0.997419\tKL_Divergence = 0.006470\n",
      "Epoch: 404\tFidelity = 0.997042\tKL_Divergence = 0.006956\n",
      "Epoch: 405\tFidelity = 0.997301\tKL_Divergence = 0.006623\n",
      "Epoch: 406\tFidelity = 0.996714\tKL_Divergence = 0.007382\n",
      "Epoch: 407\tFidelity = 0.996476\tKL_Divergence = 0.007688\n",
      "Epoch: 408\tFidelity = 0.996533\tKL_Divergence = 0.007614\n",
      "Epoch: 409\tFidelity = 0.996730\tKL_Divergence = 0.007359\n",
      "Epoch: 410\tFidelity = 0.996451\tKL_Divergence = 0.007721\n",
      "Epoch: 411\tFidelity = 0.997080\tKL_Divergence = 0.006903\n",
      "Epoch: 412\tFidelity = 0.997001\tKL_Divergence = 0.007005\n",
      "Epoch: 413\tFidelity = 0.997035\tKL_Divergence = 0.006961\n",
      "Epoch: 414\tFidelity = 0.997466\tKL_Divergence = 0.006404\n",
      "Epoch: 415\tFidelity = 0.997331\tKL_Divergence = 0.006576\n",
      "Epoch: 416\tFidelity = 0.997742\tKL_Divergence = 0.006042\n",
      "Epoch: 417\tFidelity = 0.997891\tKL_Divergence = 0.005852\n",
      "Epoch: 418\tFidelity = 0.997689\tKL_Divergence = 0.006113\n",
      "Epoch: 419\tFidelity = 0.997707\tKL_Divergence = 0.006087\n",
      "Epoch: 420\tFidelity = 0.998248\tKL_Divergence = 0.005391\n",
      "Epoch: 421\tFidelity = 0.998339\tKL_Divergence = 0.005271\n",
      "Epoch: 422\tFidelity = 0.998601\tKL_Divergence = 0.004936\n",
      "Epoch: 423\tFidelity = 0.998488\tKL_Divergence = 0.005080\n",
      "Epoch: 424\tFidelity = 0.998206\tKL_Divergence = 0.005444\n",
      "Epoch: 425\tFidelity = 0.998347\tKL_Divergence = 0.005261\n",
      "Epoch: 426\tFidelity = 0.998191\tKL_Divergence = 0.005464\n",
      "Epoch: 427\tFidelity = 0.997861\tKL_Divergence = 0.005889\n",
      "Epoch: 428\tFidelity = 0.997652\tKL_Divergence = 0.006160\n",
      "Epoch: 429\tFidelity = 0.997570\tKL_Divergence = 0.006262\n",
      "Epoch: 430\tFidelity = 0.997841\tKL_Divergence = 0.005915\n",
      "Epoch: 431\tFidelity = 0.997670\tKL_Divergence = 0.006136\n",
      "Epoch: 432\tFidelity = 0.997448\tKL_Divergence = 0.006421\n",
      "Epoch: 433\tFidelity = 0.997284\tKL_Divergence = 0.006632\n",
      "Epoch: 434\tFidelity = 0.997312\tKL_Divergence = 0.006595\n",
      "Epoch: 435\tFidelity = 0.997264\tKL_Divergence = 0.006657\n",
      "Epoch: 436\tFidelity = 0.997107\tKL_Divergence = 0.006859\n",
      "Epoch: 437\tFidelity = 0.997344\tKL_Divergence = 0.006553\n",
      "Epoch: 438\tFidelity = 0.997217\tKL_Divergence = 0.006718\n",
      "Epoch: 439\tFidelity = 0.997308\tKL_Divergence = 0.006601\n",
      "Epoch: 440\tFidelity = 0.997564\tKL_Divergence = 0.006270\n",
      "Epoch: 441\tFidelity = 0.997710\tKL_Divergence = 0.006083\n",
      "Epoch: 442\tFidelity = 0.997442\tKL_Divergence = 0.006428\n",
      "Epoch: 443\tFidelity = 0.997787\tKL_Divergence = 0.005982\n",
      "Epoch: 444\tFidelity = 0.997848\tKL_Divergence = 0.005903\n",
      "Epoch: 445\tFidelity = 0.997724\tKL_Divergence = 0.006063\n",
      "Epoch: 446\tFidelity = 0.997678\tKL_Divergence = 0.006124\n",
      "Epoch: 447\tFidelity = 0.997646\tKL_Divergence = 0.006166\n",
      "Epoch: 448\tFidelity = 0.997341\tKL_Divergence = 0.006558\n",
      "Epoch: 449\tFidelity = 0.997207\tKL_Divergence = 0.006728\n",
      "Epoch: 450\tFidelity = 0.997383\tKL_Divergence = 0.006500\n",
      "Epoch: 451\tFidelity = 0.997428\tKL_Divergence = 0.006443\n",
      "Epoch: 452\tFidelity = 0.997302\tKL_Divergence = 0.006605\n",
      "Epoch: 453\tFidelity = 0.997523\tKL_Divergence = 0.006318\n",
      "Epoch: 454\tFidelity = 0.997357\tKL_Divergence = 0.006531\n",
      "Epoch: 455\tFidelity = 0.997630\tKL_Divergence = 0.006179\n",
      "Epoch: 456\tFidelity = 0.997938\tKL_Divergence = 0.005781\n",
      "Epoch: 457\tFidelity = 0.997836\tKL_Divergence = 0.005913\n",
      "Epoch: 458\tFidelity = 0.997810\tKL_Divergence = 0.005945\n",
      "Epoch: 459\tFidelity = 0.997969\tKL_Divergence = 0.005738\n",
      "Epoch: 460\tFidelity = 0.997927\tKL_Divergence = 0.005789\n",
      "Epoch: 461\tFidelity = 0.998198\tKL_Divergence = 0.005439\n",
      "Epoch: 462\tFidelity = 0.998180\tKL_Divergence = 0.005463\n",
      "Epoch: 463\tFidelity = 0.998285\tKL_Divergence = 0.005329\n",
      "Epoch: 464\tFidelity = 0.998238\tKL_Divergence = 0.005389\n",
      "Epoch: 465\tFidelity = 0.998250\tKL_Divergence = 0.005375\n",
      "Epoch: 466\tFidelity = 0.998363\tKL_Divergence = 0.005229\n",
      "Epoch: 467\tFidelity = 0.998368\tKL_Divergence = 0.005226\n",
      "Epoch: 468\tFidelity = 0.998229\tKL_Divergence = 0.005403\n",
      "Epoch: 469\tFidelity = 0.998268\tKL_Divergence = 0.005353\n",
      "Epoch: 470\tFidelity = 0.998091\tKL_Divergence = 0.005581\n",
      "Epoch: 471\tFidelity = 0.998044\tKL_Divergence = 0.005642\n",
      "Epoch: 472\tFidelity = 0.998299\tKL_Divergence = 0.005312\n",
      "Epoch: 473\tFidelity = 0.998324\tKL_Divergence = 0.005279\n",
      "Epoch: 474\tFidelity = 0.998109\tKL_Divergence = 0.005557\n",
      "Epoch: 475\tFidelity = 0.998382\tKL_Divergence = 0.005204\n",
      "Epoch: 476\tFidelity = 0.998509\tKL_Divergence = 0.005041\n",
      "Epoch: 477\tFidelity = 0.998599\tKL_Divergence = 0.004925\n",
      "Epoch: 478\tFidelity = 0.998551\tKL_Divergence = 0.004985\n",
      "Epoch: 479\tFidelity = 0.998358\tKL_Divergence = 0.005232\n",
      "Epoch: 480\tFidelity = 0.998245\tKL_Divergence = 0.005378\n",
      "Epoch: 481\tFidelity = 0.998193\tKL_Divergence = 0.005445\n",
      "Epoch: 482\tFidelity = 0.998071\tKL_Divergence = 0.005602\n",
      "Epoch: 483\tFidelity = 0.998147\tKL_Divergence = 0.005504\n",
      "Epoch: 484\tFidelity = 0.998125\tKL_Divergence = 0.005533\n",
      "Epoch: 485\tFidelity = 0.998051\tKL_Divergence = 0.005627\n",
      "Epoch: 486\tFidelity = 0.997902\tKL_Divergence = 0.005818\n",
      "Epoch: 487\tFidelity = 0.997917\tKL_Divergence = 0.005798\n",
      "Epoch: 488\tFidelity = 0.998023\tKL_Divergence = 0.005660\n",
      "Epoch: 489\tFidelity = 0.998110\tKL_Divergence = 0.005547\n",
      "Epoch: 490\tFidelity = 0.997950\tKL_Divergence = 0.005754\n",
      "Epoch: 491\tFidelity = 0.997644\tKL_Divergence = 0.006149\n",
      "Epoch: 492\tFidelity = 0.997536\tKL_Divergence = 0.006289\n",
      "Epoch: 493\tFidelity = 0.997604\tKL_Divergence = 0.006201\n",
      "Epoch: 494\tFidelity = 0.997553\tKL_Divergence = 0.006267\n",
      "Epoch: 495\tFidelity = 0.997669\tKL_Divergence = 0.006116\n",
      "Epoch: 496\tFidelity = 0.997645\tKL_Divergence = 0.006146\n",
      "Epoch: 497\tFidelity = 0.997543\tKL_Divergence = 0.006278\n",
      "Epoch: 498\tFidelity = 0.997723\tKL_Divergence = 0.006045\n",
      "Epoch: 499\tFidelity = 0.997726\tKL_Divergence = 0.006042\n",
      "Epoch: 500\tFidelity = 0.997720\tKL_Divergence = 0.006049\n",
      "Total time elapsed during training: 9530.892 s\n"
     ]
    }
   ],
   "source": [
    "# experiment params\n",
    "#each_n_shot_list = [10, 50, 100, 500, 1000, 5000, 10000]\n",
    "each_n_shot_list = [1000]\n",
    "#error_model_list = [\"depolarizing\", \"unitary\", \"depolarizing&unitary\"]\n",
    "error_model_list = [\"unitary\"]\n",
    "\n",
    "for error_model in error_model_list:\n",
    "    for each_n_shot in each_n_shot_list:\n",
    "        print(f\"error_model : {error_model}, each_n_shot : {each_n_shot}\")\n",
    "        train_data_path = f\"./data/{error_model}/error_prob_{100*error_rate}%/num_of_data_{each_n_shot}/\"\n",
    "        meas_pattern_path = train_data_path + \"/measurement_pattern.txt\"\n",
    "        meas_label_path = train_data_path + \"/measurement_label.txt\"\n",
    "        meas_result_path = train_data_path + \"/measurement_result.txt\"\n",
    "        ideal_rho_re_path = ideal_state_path + \"/rho_real.txt\"\n",
    "        ideal_rho_im_path = ideal_state_path + \"/rho_imag.txt\"\n",
    "        #ideal_state_vector = ideal_state_path + \"/state_vector.txt\"\n",
    "        \n",
    "        meas_result, ideal_rho, meas_label, meas_pattern = data.load_data_DM(meas_result_path,\n",
    "                                                                             ideal_rho_re_path,\n",
    "                                                                             ideal_rho_im_path,\n",
    "                                                                             meas_label_path,\n",
    "                                                                             meas_pattern_path)\n",
    "        \n",
    "        nn_state_dm = DensityMatrix(num_visible = n_visible_unit, \n",
    "                                    num_hidden = n_hidden_unit, \n",
    "                                    num_aux = n_aux_unit, \n",
    "                                    unitary_dict = unitaries.create_dict(),\n",
    "                                    gpu = False)\n",
    "        \n",
    "        callbacks = create_callback_dm(nn_state_dm)\n",
    "        \n",
    "        nn_state_dm.fit(data = meas_result,\n",
    "                        input_bases = meas_label,\n",
    "                        epochs = epoch,\n",
    "                        pos_batch_size = pbs,\n",
    "                        neg_batch_size = nbs,\n",
    "                        lr = lr,\n",
    "                        k = n_gibbs_step,\n",
    "                        bases = meas_pattern,\n",
    "                        callbacks = callbacks,\n",
    "                        time = True,\n",
    "                        optimizer = torch.optim.Adadelta,\n",
    "                        scheduler = torch.optim.lr_scheduler.StepLR,\n",
    "                        scheduler_args = {\"step_size\": lr_drop_epoch, \"gamma\": lr_drop_factor},\n",
    "                       )\n",
    "        \n",
    "        # save model\n",
    "        nn_state_dm.save(f\"./exp_max_eigen_vector_varying_n_shot/model_n_pattern_shot={each_n_shot}_{error_model}.pt\")\n",
    "        # save train log\n",
    "        train_log_df = pd.DataFrame()\n",
    "        train_log_df[\"epoch\"] = np.arange(period, epoch+1, period)\n",
    "        train_log_df[\"Fidelity\"] = callbacks[0][\"Fidelity\"]\n",
    "        train_log_df[\"KL_Divergence\"] = callbacks[0][\"KL_Divergence\"]\n",
    "        #train_log_df[\"Observable_XX_ev\"] = callbacks[0][\"Observable_XX_ev\"]\n",
    "        #train_log_df[\"Observable_ZZ_ev\"] = callbacks[0][\"Observable_ZZ_ev\"]\n",
    "        train_log_df.to_csv(f\"./exp_max_eigen_vector_varying_n_shot/model_n_pattern_shot={each_n_shot}_{error_model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f33aa-9a12-44aa-9ed6-fe732e87efc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

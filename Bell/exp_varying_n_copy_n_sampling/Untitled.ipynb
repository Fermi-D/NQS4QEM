{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4579476d-fd92-4516-9b66-b942e7c28212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ab23fbbb7c477db4b09dba707a10a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80eec733df534db68de60aaa983ca161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_copy : 5, n_sampling : 100\n",
      "n_copy : 4, n_sampling : 100\n",
      "n_copy : 3, n_sampling : 100\n",
      "n_copy : 2, n_sampling : 100\n",
      "n_copy : 1, n_sampling : 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a671b04f670f4b87b74395fa63a16407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_copy : 5, n_sampling : 1000\n",
      "n_copy : 4, n_sampling : 1000\n",
      "n_copy : 3, n_sampling : 1000\n",
      "n_copy : 2, n_sampling : 1000\n",
      "n_copy : 1, n_sampling : 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a53f3ed0b3842be92fe77bb016a458d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_copy : 5, n_sampling : 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 226>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m     ev_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_exec):\n\u001b[0;32m--> 232\u001b[0m         ev_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalculate_distilled_expectation_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_copy\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    233\u001b[0m     each_sampling_ev_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_copy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_copy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ev_list\n\u001b[1;32m    234\u001b[0m each_sampling_ev_df\u001b[38;5;241m.\u001b[39mto_csv( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./depolarizing&unitary/ev_n_sampling=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_sampling\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mcalculate_distilled_expectation_value\u001b[0;34m(pauli_dict, num_samples, num_copies)\u001b[0m\n\u001b[1;32m    168\u001b[0m obs_div \u001b[38;5;241m=\u001b[39m GeneralPauliDistill({}, num_copies)\n\u001b[1;32m    169\u001b[0m num_stat \u001b[38;5;241m=\u001b[39m obs_num\u001b[38;5;241m.\u001b[39mstatistics(nn_state_dm, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n\u001b[0;32m--> 170\u001b[0m div_stat \u001b[38;5;241m=\u001b[39m \u001b[43mobs_div\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_state_dm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muncertainties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ufloat\n\u001b[1;32m    173\u001b[0m num \u001b[38;5;241m=\u001b[39m ufloat(num_stat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m], num_stat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd_error\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/qucumber/observables/observable.py:196\u001b[0m, in \u001b[0;36mObservableBase.statistics\u001b[0;34m(self, nn_state, num_samples, num_chains, burn_in, steps, initial_state, overwrite)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_time_steps):\n\u001b[1;32m    194\u001b[0m     num_gibbs_steps \u001b[38;5;241m=\u001b[39m burn_in \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m steps\n\u001b[0;32m--> 196\u001b[0m     chains \u001b[38;5;241m=\u001b[39m \u001b[43mnn_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gibbs_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     sample_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_from_samples(nn_state, chains)\n\u001b[1;32m    205\u001b[0m     running_mean, running_variance, running_length \u001b[38;5;241m=\u001b[39m _update_statistics(\n\u001b[1;32m    206\u001b[0m         running_mean,\n\u001b[1;32m    207\u001b[0m         running_variance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m         num_chains,\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/qucumber/nn_states/neural_state.py:131\u001b[0m, in \u001b[0;36mNeuralStateBase.sample\u001b[0;34m(self, k, num_samples, initial_state, overwrite)\u001b[0m\n\u001b[1;32m    126\u001b[0m     sample_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize((num_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_visible))\n\u001b[1;32m    127\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample(sample_size)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    128\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrbm_am\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgibbs_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/qucumber/rbm/purification_rbm.py:333\u001b[0m, in \u001b[0;36mPurificationRBM.gibbs_steps\u001b[0;34m(self, k, initial_state, overwrite)\u001b[0m\n\u001b[1;32m    330\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m*\u001b[39mv\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_aux)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_W)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_h_given_v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_a_given_v(v, out\u001b[38;5;241m=\u001b[39ma)\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_v_given_ha(h, a, out\u001b[38;5;241m=\u001b[39mv)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/qucumber/rbm/purification_rbm.py:288\u001b[0m, in \u001b[0;36mPurificationRBM.sample_h_given_v\u001b[0;34m(self, v, out)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sample/generate a hidden state given a visible state\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m:param v: The visible state\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m:rtype: torch.Tensor\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_h_given_v(v, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m--> 288\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from qucumber.nn_states import DensityMatrix\n",
    "from qucumber.nn_states import ComplexWaveFunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "import qucumber.utils.unitaries as unitaries\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.cplx as cplx\n",
    "import qucumber.utils.data as data\n",
    "from qucumber.observables import ObservableBase, to_pm1\n",
    "from qucumber.observables.pauli import flip_spin\n",
    "import qucumber\n",
    "\n",
    "from qulacs.gate import Pauli\n",
    "\n",
    "from numba import jit\n",
    "from numba.experimental import jitclass\n",
    "\n",
    "# data path info\n",
    "environment = \"local\"\n",
    "#if environment == \"local\":\n",
    "    #with open('./params_setting.yaml', 'r') as yml:\n",
    "        #params = yaml.safe_load(yml)\n",
    "if environment == \"colab\":\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "    drive_path = \"/content/drive/MyDrive/NQS4QEM/Bell\"\n",
    "    #with open(drive_path + '/params_setting.yaml', 'r') as yml:\n",
    "        #params = yaml.safe_load(yml)\n",
    "\n",
    "#if environment == \"local\":\n",
    "    #train_data_path = f\"./data/{error_model}/error_prob_{100*error_rate}%/num_of_data_{each_n_shot}\"\n",
    "    #ideal_state_path = f\"./target_state\"\n",
    "if environment == \"colab\":\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "    drive_path = \"/content/drive/MyDrive/NQS4QEM/Bell\"\n",
    "    #train_data_path = drive_path + f\"/data/{error_model}/error_prob_{100*error_rate}%/num_of_data_{each_n_shot}\"\n",
    "    #ideal_state_path = drive_path + f\"/target_state\"\n",
    "\n",
    "\n",
    "# settings\n",
    "## warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "## seaborn layout\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "## seed\n",
    "def seed_settings(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    qucumber.set_random_seed(seed, cpu=True, gpu=True)\n",
    "\n",
    "seed_settings(seed=42)\n",
    "\n",
    "class GeneralPauliDistill(ObservableBase):\n",
    "    def __init__(self, pauli_dict: dict, m: int) -> None:\n",
    "        self.name = \"distilled_pauli\"\n",
    "        self.symbol = \"distilled_general_pauli\"\n",
    "        self.pauli_dict = pauli_dict\n",
    "        self.num_copy = m\n",
    "\n",
    "    def apply(self, nn_state, samples):\n",
    "        \"\"\"\n",
    "        This function calcualte <x1 x2 ... xm | rho^{\\otimes m} O | xm x1 x2 ... xm-1> / <x1 x2 ... xm | rho^{\\otimes m} | x1 x2 ... xm>\n",
    "        where O acts only on the first register.\n",
    "        \"\"\"\n",
    "\n",
    "        # [num_sample, num_visible_node]\n",
    "        # samples = [s1, s2, s3 ... sN]\n",
    "        #  where num_sample = N, and si is num_visible_node-bits\n",
    "        samples = samples.to(device=nn_state.device)\n",
    "\n",
    "        num_sample, num_visible_node = samples.shape\n",
    "\n",
    "        # [num_sample, num_visible_node * num_copy]\n",
    "        # samples_array = [[s1 sN sN-1], [s2 s1 sN], [s3 s2 s1],.. [sN sN-1 sN-2]]\n",
    "        #  each row is num_copy*num_visible_node bits the above example is for num_copy=3\n",
    "        samples_array = []\n",
    "        for copy_index in range(self.num_copy):\n",
    "            rolled_samples = torch.roll(samples, shifts=copy_index, dims=0)\n",
    "            samples_array.append(rolled_samples)\n",
    "        samples_array = torch.hstack(samples_array)\n",
    "        assert(samples_array.shape[0] == num_sample)\n",
    "        assert(samples_array.shape[1] == num_visible_node * self.num_copy)\n",
    "\n",
    "        # roll second dim of [num_sample, num_visible_node * num_copy] by num_visible_node\n",
    "        # swapped_samples_array = [[sN-1 s1 sN], [sN s2 s1], [s1 s3 s2],.. [sN-2 sN sN-1]]\n",
    "        swapped_samples_array = torch.roll(samples_array, shifts = num_visible_node, dims=1)\n",
    "\n",
    "        # pick copy of first block\n",
    "        #  first_block_sample = [sN-1, sN, s1, s2, ... sN-2]\n",
    "        first_block_sample = swapped_samples_array[:, :num_visible_node].clone()\n",
    "\n",
    "        # calculate coefficient for first block [num_samples, 0:num_visible_node]\n",
    "        total_prod = cplx.make_complex(torch.ones_like(samples[:,0]), torch.zeros_like(samples[:,0]))\n",
    "        for index, pauli in self.pauli_dict.items():\n",
    "            assert(index < num_visible_node)\n",
    "            coeff = to_pm1(first_block_sample[:, index])\n",
    "            if pauli == \"Z\":\n",
    "                coeff = cplx.make_complex(coeff, torch.zeros_like(coeff))\n",
    "                total_prod = cplx.elementwise_mult(coeff, total_prod)\n",
    "            elif pauli == \"Y\":\n",
    "                coeff = cplx.make_complex(torch.zeros_like(coeff), coeff)\n",
    "                total_prod = cplx.elementwise_mult(coeff, total_prod)\n",
    "\n",
    "        # flip samples for for first block [num_samples, 0:num_visible_node]\n",
    "        # first_block_sample -> [OsN-1, OsN, Os1, Os2, ... OsN-2]\n",
    "        #  where Osi is bit array after Pauli bit-flips\n",
    "        for index, pauli in self.pauli_dict.items():\n",
    "            assert(index < num_visible_node)\n",
    "            if pauli in [\"X\", \"Y\"]:\n",
    "                first_block_sample = flip_spin(index, first_block_sample)\n",
    "\n",
    "\n",
    "        # store flipped first block\n",
    "        swapped_samples_array[:, :num_visible_node] = first_block_sample\n",
    "\n",
    "        # calculate product of coefficients\n",
    "        # samples_array = [[s1 sN sN-1], [s2 s1 sN], [s3 s2 s1],.. [sN sN-1 sN-2]]\n",
    "        # swapped_samples_array = [[OsN-1 s1 sN], [OsN s2 s1], [Os1 s3 s2],.. [OsN-2 sN sN-1]]\n",
    "        \"\"\"\n",
    "        total_prod = [\n",
    "            <s1 sN sN-1 | rho^{\\otimes 3} | OsN-1 s1 sN> / <s1 sN sN-1 | rho^{\\otimes 3} | s1 sN sN-1> ,\n",
    "            <s2 s1 sN   | rho^{\\otimes 3} | OsN s2 s1>   / <s2 s1 sN   | rho^{\\otimes 3} | s2 s1 sN> ,\n",
    "            <s3 s2 s1   | rho^{\\otimes 3} | Os1 s3 s2>   / <s3 s2 s1   | rho^{\\otimes 3} | s3 s2 s1> ,\n",
    "\n",
    "        e.g.\n",
    "        <s3 s2 s1   | rho^{\\otimes 3} | Os1 s3 s2>   / <s3 s2 s1   | rho^{\\otimes 3} | s3 s2 s1>\n",
    "         = <s3 | rho | Os1> <s2 | rho | s3> < s1| rho | s2> / (<s3 | rho | s3> <s2 | rho | s2> < s1| rho | s1>)\n",
    "         =  (<s3 | rho | Os1> / <s3 | rho | s3>)\n",
    "          * (<s2 | rho | s3> / <s2 | rho | s2> )\n",
    "          * (< s1| rho | s2> / < s1| rho | s1>)\n",
    "\n",
    "        importance_sampling_numerator(s3, Os1)  provides <s3 | rho | Os1>\n",
    "        importance_sampling_denominator(s3)     provides <s3 | rho | s3>\n",
    "        \"\"\"\n",
    "        for copy_index in range(self.num_copy):\n",
    "            st = copy_index * samples.shape[1]\n",
    "            en = (copy_index+1) * samples.shape[1]\n",
    "            # numerator is []\n",
    "            numerator = nn_state.importance_sampling_numerator(swapped_samples_array[:, st:en], samples_array[:, st:en])\n",
    "            denominator = nn_state.importance_sampling_denominator(samples_array[:, st:en])\n",
    "            values = cplx.elementwise_division(numerator, denominator)\n",
    "            total_prod = cplx.elementwise_mult(total_prod, values)\n",
    "\n",
    "        value = cplx.real(total_prod)\n",
    "        return value\n",
    "\n",
    "def calculate_distilled_expectation_value(pauli_dict: dict, num_samples: int, num_copies: int):\n",
    "    obs_num = GeneralPauliDistill(pauli_dict, num_copies)\n",
    "    obs_div = GeneralPauliDistill({}, num_copies)\n",
    "    num_stat = obs_num.statistics(nn_state_dm, num_samples=num_samples)\n",
    "    div_stat = obs_div.statistics(nn_state_dm, num_samples=num_samples)\n",
    "\n",
    "    from uncertainties import ufloat\n",
    "    num = ufloat(num_stat[\"mean\"], num_stat[\"std_error\"])\n",
    "    div = ufloat(div_stat[\"mean\"], div_stat[\"std_error\"])\n",
    "    val = num/div\n",
    "    result_dict = {\"mean\": val.n , \"std_error\": val.s, \"num_samples\": num_samples, \"num_copies\": num_copies}\n",
    "    return result_dict\n",
    "\n",
    "def get_density_matrix(nn_state):\n",
    "    space = nn_state.generate_hilbert_space()\n",
    "    Z = nn_state.normalization(space)\n",
    "    tensor = nn_state.rho(space, space)/Z\n",
    "    matrix = cplx.numpy(tensor)\n",
    "    return matrix\n",
    "\n",
    "def get_max_eigvec(matrix):\n",
    "    e_val, e_vec = np.linalg.eigh(matrix)\n",
    "    me_val = e_val[-1]\n",
    "    me_vec = e_vec[:,-1]\n",
    "    return me_vec\n",
    "\n",
    "def get_eigvec(nn_state, obs, space, **kwargs):\n",
    "    dm = get_density_matrix(nn_state)\n",
    "    ev = get_max_eigvec(dm)\n",
    "    ev = np.atleast_2d(ev)\n",
    "    val = ev@obs@ev.T.conj()\n",
    "    val = val[0,0].real\n",
    "    return val\n",
    "\n",
    "def observable_XX():\n",
    "    target_list = [0, 1]\n",
    "    pauli_index = [1, 1] # 1:X , 2:Y, 3:Z\n",
    "    gate = Pauli(target_list, pauli_index) # = X_1 X_2\n",
    "    return gate.get_matrix()\n",
    "\n",
    "def observable_XX_ev(nn_state, **kwargs):\n",
    "    obs_stat = calculate_distilled_expectation_value({0: \"X\", 1: \"X\"}, n_sampling, n_copy)\n",
    "    return obs_stat[\"mean\"]\n",
    "\n",
    "def observable_XX_var(nn_state, **kwargs):\n",
    "    obs_stat = calculate_distilled_expectation_value({0: \"X\", 1: \"X\"}, n_sampling, n_copy)\n",
    "    return obs_stat[\"std_error\"]**2\n",
    "\n",
    "# experiment params\n",
    "n_copy_list = np.arange(1, 6)[::-1]\n",
    "n_sampling_list = [100, 1000, 10000, 100000]\n",
    "#error_model_list = [\"depolarizing\", \"unitary\", \"depolarizing&unitary\"]\n",
    "\n",
    "# number of execution\n",
    "n_exec = 1000\n",
    "\n",
    "# load_model\n",
    "nn_state_dm = DensityMatrix.autoload(\"./model_n_pattern_shot=1000_depolarizing_unitary.pt\", gpu=True)\n",
    "\n",
    "# save result\n",
    "for n_sampling in tqdm(n_sampling_list):\n",
    "    each_sampling_ev_df = pd.DataFrame(columns=list(map(lambda s: \"n_copy=\"+s, [str(i) for i in n_copy_list])))\n",
    "    for n_copy in tqdm(n_copy_list):\n",
    "        print(f\"n_copy : {n_copy}, n_sampling : {n_sampling}\")\n",
    "        ev_list = []\n",
    "        for i in range(n_exec):\n",
    "            ev_list.append(calculate_distilled_expectation_value({0: \"X\", 1: \"X\"}, n_sampling, n_copy)[\"mean\"])\n",
    "        each_sampling_ev_df[f\"n_copy={n_copy}\"] = ev_list\n",
    "    each_sampling_ev_df.to_csv( f\"./depolarizing&unitary/ev_n_sampling={n_sampling}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507dcbed-b9ec-439e-821f-bbafe0a132ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
